
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Invertible Neural Networks &#8212; Machine Learning for Natural Hazards</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '18_INN';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="README.html">
  
  
  
  
  
  
    <p class="title logo__title">Machine Learning for Natural Hazards</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="README.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Background and Refresher</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_intro.html">Introduction to Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_linalg.html">Linear Algebra refresher</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_gradopt.html">Gradient-based optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_linreg.html">Linear and Logistic Regression</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Neural Networks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="05_nn.html">Basics of Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_nn.html">More on Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_bestpractice.html">Best practices in the training of Machine Learning models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Convolutional Neural Networks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="08_gradopt1.html">More on gradient-based optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_cnn.html">Convolutional Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_cnnarch.html">CNNs Popular Architectures</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/ashokdahal/GeoAI4Haz" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/ashokdahal/GeoAI4Haz/issues/new?title=Issue%20on%20page%20%2F18_INN.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/18_INN.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Invertible Neural Networks</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normalizing-flows">Normalizing flows</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#coupling-flows">Coupling flows</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-coupling-function">The coupling function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-split-function">The split function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-scale-architecture">Multi-scale architecture</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reversible-network-architectures">Reversible network architectures</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#revnet-reversible-resnet">RevNet: reversible ResNet</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#i-unet-invertible-unet">i-UNet: invertible UNet</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further reading</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="invertible-neural-networks">
<h1>Invertible Neural Networks<a class="headerlink" href="#invertible-neural-networks" title="Link to this heading">#</a></h1>
<p>Invertible Neural Networks (INNs) are a class of neural networks where the input of the network can be reconstructed exactly from the output. Popular neural network
architectures such as (V)AE, UNet and ResNet are not invertible for a number of reasons. Firstly, these networks have layers that map to different dimensions by either
expanding or shrinking (mostly shrinking) the dimension of the current hidden layer to project the input into the so-called latent space. It was long believed that this
projection into a lower dimensional space is what made neural networks so powerful; invertible neural networks break with this notion. Additionally, the layers mostly carry
out convolutions, which, even if they map to a space of the same dimension, are generally rank deficient and therefore non-invertible. Moreover, popular architectures
generally incorporate operations like batch normalization or average or max pooling that are non-invertible. On top of that, some popular activation functions, like ReLU, suffer the same issue
of non-invertibility. Given that non-invertible operations like pooling, batch normalization and non-invertible activation functions are typically chosen because
they greatly improve performance, discarding them just to ensure invertibility is undesirable. Therefore, constructing an INN that performs on par with modern architectures is not
straightforward.</p>
<p>In this lecture we will cover how to construct INNs that perform on par with state-of-the-art neural network architectures and show some of their applications. INNs have two main applications
that we will cover in this lecture:</p>
<ol class="arabic simple">
<li><p>INNs are used as generative models, mostly known under the name of <em>normalizing flows (NFs)</em>. The idea is that a complicated distribution can be transformed into a
Gaussian distribution through a sequence of invertible and differentiable transformations, also called <em>flow</em>, hence the name <em>normalizing flow</em>.</p></li>
<li><p>INNs are used to overcome the memory requirements of neural networks due to storing all activations that are needed for backpropagation. In INNs, since the input can be computed from
the output, these values need not be stored and the memory requirements are constant as the size of the network increases.</p></li>
</ol>
<section id="normalizing-flows">
<h2>Normalizing flows<a class="headerlink" href="#normalizing-flows" title="Link to this heading">#</a></h2>
<p>Normalizing flows are used for generative modeling by transforming a sample distribution into the target distribution through a series of invertible transformations, called
<em>flow</em>. This principle is illustrated in the figure below</p>
<p><img alt="flow" src="_images/flow.png" /></p>
<p>Any sample <span class="math notranslate nohighlight">\(x\)</span> from the target distribution can be transformed to a sample from the base distribution <span class="math notranslate nohighlight">\(z\)</span> via the relation <span class="math notranslate nohighlight">\(f(x) = z\)</span> and if <span class="math notranslate nohighlight">\(f\)</span> is invertible then we can equivalently obtain
<span class="math notranslate nohighlight">\(x = f^{-1}(z)\)</span>. When the flow consists of multiple transformations like in the figure above, then <span class="math notranslate nohighlight">\(f = f_1 \circ f_2 \circ f_3\)</span> and <span class="math notranslate nohighlight">\(f^{-1} = f_3^{-1} \circ f_2^{-1} \circ f_1^{-1}\)</span>.
Ideally, the sample distribution is a simple one whose parameters are known and is one that is easy to sample from. When one probability density function is related to
another via a flow the relationship between the two is given by</p>
<div class="math notranslate nohighlight">
\[ 
    p_X(x) = p_Z(f(x))\vert \det Df(x)\vert
\]</div>
<p>Ideally, when looking for a probability distribution that best fits the data one is interested in minimizing the negative log-likelihood. In the case of INNs this is straightforward,
since the log-likelihood of <span class="math notranslate nohighlight">\(p_X\)</span> is related to the log-likelihood of <span class="math notranslate nohighlight">\(p_Z\)</span> via</p>
<div class="math notranslate nohighlight">
\[
    -\log p_X(x) = - \log p_Z(f(x)) - \log(\vert \det Df(x)\vert).
\]</div>
<p>This shows one benefit of INNs as compared to other generative models such as GANs and VAEs, that are not able to minimize the log-likelihood. GANs do not act on the log-likelihood and
VAEs only minimize an upper bound, see the lecture on VAEs. Moreover, by design both sampling from the distribution and inference are easy. This makes INNs well-suited for Variational Inference (VI),
as discussed in the lecture on VAEs.</p>
<p>Clearly, in order to efficiently evaluate the sought after distribution <span class="math notranslate nohighlight">\(p_X\)</span>, we need to be able to evaluate <span class="math notranslate nohighlight">\(\det Df(x)\)</span> efficiently. For a general matrix evaluating the
determinant is costly; roughly equal to the cost of inverting the matrix. As an example, assume that we have a neural network that maps <span class="math notranslate nohighlight">\(\mathbb{R}^n \to \mathbb{R}^n\)</span>. Choosing
a sigmoid activation function yields the following expression for one forward pass from one layer to the next</p>
<div class="math notranslate nohighlight">
\[
    \sigma(x) = \frac{1}{1 + e^{- Wx - b}},
\]</div>
<p>where the exponential is evaluated pointwise. The gradient is given by</p>
<div class="math notranslate nohighlight">
\[
    D\sigma(x) = \begin{bmatrix} \frac{e^{-Wx - b}}{(1 + e^{-Wx - b})^2}\odot w_1, \ldots, \frac{e^{-Wx - b}}{(1 + e^{-Wx - b})^2}\odot w_n \end{bmatrix}.
\]</div>
<p>Clearly, evaluating the determinant of this matrix is not feasible for large-scale problems, even though the network itself has an invertible structure.
When the matrix has special structure, e.g. diagonal or triangular, or if the matrix is unitary, computing the determinant is cheap. However, guaranteeing a certain structure
or property of the weight matrix is either not possible, computationally expensive, or severely limits the expressive capabilities of the network. <a class="reference external" href="https://arxiv.org/abs/1410.8516">Dinh et al., 2014</a>
proposed the <em>coupling flow</em> that is both invertible and has a determinant that is cheap to evaluate.</p>
</section>
<section id="coupling-flows">
<h2>Coupling flows<a class="headerlink" href="#coupling-flows" title="Link to this heading">#</a></h2>
<p>A coupling flow is a flow that splits the input into two parts, say <span class="math notranslate nohighlight">\(x_A\)</span> and <span class="math notranslate nohighlight">\(x_B\)</span>, after which <span class="math notranslate nohighlight">\(x_A\)</span> is mapped directly to the output via the identity function and <span class="math notranslate nohighlight">\(x_B\)</span> undergoes an invertible
transformation, which is conditioned on <span class="math notranslate nohighlight">\(x_A\)</span>. The principle is outlined in the figure below.</p>
<p><img alt="coupling_flow" src="_images/coupling_flow.png" /></p>
<p>Here, <span class="math notranslate nohighlight">\(f\)</span> denotes the <em>coupling function</em> that ensures the dependency of the output <span class="math notranslate nohighlight">\(z_B\)</span> on the input <span class="math notranslate nohighlight">\(x_A\)</span>. <span class="math notranslate nohighlight">\(\theta\)</span> can be any function and need not even be invertible,
since it only parametrizes the coupling function <span class="math notranslate nohighlight">\(f\)</span>, and it can be computed from the output using the equality <span class="math notranslate nohighlight">\(z_A = x_A\)</span>. For example, in <a class="reference external" href="https://arxiv.org/abs/1410.8516">Dinh et al., 2014</a> the authors
use a simple ReLU MLP, <a class="reference external" href="https://arxiv.org/pdf/1605.08803.pdf">Dinh et al., 2016</a> use a convolutional residual neural network, and in <a class="reference external" href="https://arxiv.org/pdf/1807.03039.pdf">Kingma et al., 2018</a> a three-layer
CNN with ReLU activations. As illustrated in the figure below, the coupling flow is invertible.</p>
<p><img alt="coupling_flow_inv" src="_images/coupling_flow_inv.png" /></p>
<p>By splitting the input the Jacobian will consist of four components. Denoting the function mapping the input <span class="math notranslate nohighlight">\(x\)</span> to the output <span class="math notranslate nohighlight">\(z\)</span> by <span class="math notranslate nohighlight">\(g\)</span> we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    Dg(x) = \begin{bmatrix} \frac{\partial z_A}{\partial x_A} &amp; \frac{\partial z_A}{\partial x_A} \\ \frac{\partial z_B}{\partial x_A} &amp; \frac{\partial z_B}{\partial x_B}\end{bmatrix} = 
            \begin{bmatrix} I &amp; 0 \\ \frac{\partial}{\partial x_A} f(x_B;\theta(x_A)) &amp; Df(x_B;\theta(x_A)) \end{bmatrix}.
\end{split}\]</div>
<p>The Jacobian is a triangular matrix and therefore the determinant is easy to evaluate since it is simply the product of the diagonal elements.</p>
<section id="the-coupling-function">
<h3>The coupling function<a class="headerlink" href="#the-coupling-function" title="Link to this heading">#</a></h3>
<p>The coupling flow was introduced in <a class="reference external" href="https://arxiv.org/abs/1410.8516">Dinh et al., 2014</a>, where the additive coupling function has the following structure
$<span class="math notranslate nohighlight">\(
    \begin{aligned}
        z_A = x_A \\
        z_B = x_B + \theta(x_A)
    \end{aligned} 
\)</span><span class="math notranslate nohighlight">\(
In subsequent work [Dinh et al., 2016](https://arxiv.org/pdf/1605.08803.pdf) used the affine coupling function
\)</span><span class="math notranslate nohighlight">\(
    \begin{aligned}
        z_A = x_A \\
        z_B = x_B \odot e^{s(x_A)} + t(x_A),
    \end{aligned} 
\)</span><span class="math notranslate nohighlight">\(
where \)</span>s<span class="math notranslate nohighlight">\( denotes a scaling function and \)</span>t$ a translation function.</p>
</section>
<section id="the-split-function">
<h3>The split function<a class="headerlink" href="#the-split-function" title="Link to this heading">#</a></h3>
<p>Splitting the input can be done a number of ways. <a class="reference external" href="https://arxiv.org/pdf/1605.08803.pdf">Dinh et al., 2016</a> split the input along the spatial dimension using a checkerboard pattern,
after which a squeezing operation is applied that reduces the spatial dimension and accordingly increases the number of channels. Subsequently, the channel dimension is masked in a
manner that doesn’t interfere with the masking in the spatial dimension. This principle is illustrated in the figure below that is directly taken from the paper.</p>
<p><img alt="splitting" src="_images/splitting_RealNVP.png" /></p>
<p>In this figure, the white squares are fed directly to the output whereas the black squares are fed through an invertible function conditioned on the white squares.
By splitting the input the same way every time certain parts of the input are never used but only propagated directly to the output. To make sure all components are leveraged, the
intermediate outputs have to be shuffled. <a class="reference external" href="https://arxiv.org/pdf/1605.08803.pdf">Dinh et al., 2016</a> propose alternating shuffling where the components that are unaltered
are fed through the invertible function in the next iteration and vice versa. Alternatively, <a class="reference external" href="https://arxiv.org/pdf/1807.03039.pdf">Kingma et al., 2018</a> propose the use of invertible
learned 1-x-1 convolutions, and show that this improves performance compared to alternating shuffling of <a class="reference external" href="https://arxiv.org/pdf/1605.08803.pdf">Dinh et al., 2016</a> or random shuffling.</p>
</section>
<section id="multi-scale-architecture">
<h3>Multi-scale architecture<a class="headerlink" href="#multi-scale-architecture" title="Link to this heading">#</a></h3>
<p>Along with the spatial splitting of the input <a class="reference external" href="https://arxiv.org/pdf/1605.08803.pdf">Dinh et al., 2016</a> propose a multi-scale architecture where the spatial dimension is downsampled followed
by a corresponding increase in the number of channels. Their overal architecture combines 3 coupling layers with spatial checkerboard masking followed by a squeezing operation with channel-wise
masking. Because the layers of an INN preserve dimension, propagating the input through the network is costly both in terms of computational cost and memory cost. Therefore, half of the dimension
are fed through the network directly without undergoing any more coupling functions.</p>
</section>
</section>
<section id="reversible-network-architectures">
<h2>Reversible network architectures<a class="headerlink" href="#reversible-network-architectures" title="Link to this heading">#</a></h2>
<p>Due to the reversible nature of the network INNs have low memory cost. To understand this, let’s recall the algorithm that is used to perform gradient descent for neural networks: backpropagation.
Backpropagation essentially computes the gradient by repeated application of the chain rule. Recall the following figure from lecture 6:</p>
<p><img alt="BACKPROP_NN1" src="_images/backprop_nn1.png" /></p>
<p>To compute the derivative we need access to result of the activation functions. If the network is fully invertible these values can be computed from the output. However, if the network is not invertible
either the values have to be recomputed, which is extremely expensive for large networks, or the output of the activations has to be stored. This is essentially what happens in backpropagation. The drawback
is that all the intermediate outputs have to be stored, putting a huge burden on the memory. Since GPUs generally have limited memory this becomes a bottleneck for deep neural networks. When the network is
reversible, the inputs can be computed from the outputs which lifts the burden from the memory in exchange computation. This is generally a favorable trade for GPUs. A number of popular architectures
now have reversible or invertible counterparts, for example UNet and ResNet. Note that, in order to lift the memory burden from backpropagation, the network need not be invertible: injectivity suffices. We now
show two popular network architectures that can be made invertible: UNet and ResNet.</p>
<section id="revnet-reversible-resnet">
<h3>RevNet: reversible ResNet<a class="headerlink" href="#revnet-reversible-resnet" title="Link to this heading">#</a></h3>
<p>The ResNet architecture is characterized by skip connections and consists of residual blocks of the form</p>
<div class="math notranslate nohighlight">
\[
    y = x + \mathcal{F}(x),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{F}(x)\)</span> denotes the residual block. The RevNet uses a coupling flow that is slightly different from the previous coupling as shown in the figure below from the RevNet paper <a class="reference external" href="https://arxiv.org/pdf/1707.04585.pdf">Gomez et al., 2017</a>.</p>
<p><img alt="revnet" src="_images/RevNet.png" /></p>
<p>Here, both <span class="math notranslate nohighlight">\(F\)</span> and <span class="math notranslate nohighlight">\(G\)</span> denote the residual blocks that are typical for the standard ResNet. The coupling flow is given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{aligned}
        z_A &amp; = x_A + F(x_B) \\
        z_B &amp; = x_B + G(z_A)
    \end{aligned}
\end{split}\]</div>
<p>with inverse</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{aligned}
        x_B &amp; = z_B - G(z_A) \\
        x_A &amp; = z_A - F(x_B)
    \end{aligned}
\end{split}\]</div>
</section>
<section id="i-unet-invertible-unet">
<h3>i-UNet: invertible UNet<a class="headerlink" href="#i-unet-invertible-unet" title="Link to this heading">#</a></h3>
<p>The UNet architecture calculates features on multiple scales by downsampling the input a number of times: this is the depth of the UNet. Every downsampling layers is followed by a number of convolutional layers that extract the
features at the current scale. When the maximum downsampling is reached, the input is upsampled at the same rate until an output with the same dimension as the input is reached. To combine the extracted features from different scales
the UNet passes the downsampled images directly from the downsampling layers to the upsampling layers where they are concatenated. This is illustrated in the figure below.</p>
<p><img alt="unet" src="_images/unet.png" /></p>
<p>The convolutional layers in the UNet can be replaced by the invertible coupling layers to make them invertible. The downsampling layers can be made invertible by increasing the number of channels. If we denote the size of the current image
by <span class="math notranslate nohighlight">\(h \times w \times c\)</span>, where <em>h</em> denotes the height, <em>w</em> denotes the width and <em>c</em> denotes the number of channels, then a map from <span class="math notranslate nohighlight">\(\mathbb{R}^{h \times w \times c} \to \mathbb{R}^{h/n_h \times w/n_w \times n_h\cdot n_w\cdot c}\)</span> can be made
invertible. In principle one could use the downsampling operation according to the checkerboard pattern as introduced by <a class="reference external" href="https://arxiv.org/pdf/1605.08803.pdf">Dinh et al., 2016</a>. However, the corresponding upsampling operation introduces checkerboard
artifacts. <a class="reference external" href="https://arxiv.org/pdf/2005.05220.pdf">Etmann et al., 2020</a> proposed the use of learned orthogonal downsampling and upsampling operations. The key idea is that the downsampling operation can be expressed as a convolution where the kernel
size equals the stride. This way, convolution can be seen as matrix-vector multiplication with a convolutional kernel matrix that has the dimension of the number of channels. This principle is illustrated in the figure below.</p>
<p><img alt="conv_full" src="_images/conv_full.png" /></p>
<p>Note that it is convenient but not strictly necessary for the kernel matrix to be orthogonal. Orthogonality makes the subsequent upsampling operator easy to evaluate, as it’s just the adjoint of the kernel matrix. The invertible UNet, i-UNet, is now constructed
by combining the invertible downsampling operator with the coupling functions we have seen before, replacing the standard downsampling and convolutional layers respectively. The i-UNet architecture is shown in the figure below from the paper
by <a class="reference external" href="https://arxiv.org/pdf/2005.05220.pdf">Etmann et al., 2020</a>.</p>
<p><img alt="iunet" src="_images/i-UNet.png" /></p>
</section>
</section>
<section id="further-reading">
<h2>Further reading<a class="headerlink" href="#further-reading" title="Link to this heading">#</a></h2>
<p>These notes were heavily inspired by the following tutorials:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=8XufsgG066A">Brubacker and Kothe</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=IpbeIwSr7r0">Paul Hand</a></p></li>
</ul>
<p>Below are the references for the RealNVP, GLOW, i-UNet and RevNet papers:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/1605.08803">RealNVP</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1807.03039">GLOW</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/2005.05220.pdf">i-UNet</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1707.04585.pdf">RevNet</a></p></li>
</ul>
<p>Library for building INNs:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/silvandeleemput/memcnn">MemCNN</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normalizing-flows">Normalizing flows</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#coupling-flows">Coupling flows</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-coupling-function">The coupling function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-split-function">The split function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-scale-architecture">Multi-scale architecture</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reversible-network-architectures">Reversible network architectures</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#revnet-reversible-resnet">RevNet: reversible ResNet</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#i-unet-invertible-unet">i-UNet: invertible UNet</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further reading</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Ashok Dahal, Modified from the Content of Matteo Ravasi, KAUST
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>