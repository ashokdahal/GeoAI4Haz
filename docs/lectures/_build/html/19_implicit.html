
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Implicit neural networks &#8212; Machine Learning for Natural Hazards</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '19_implicit';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="README.html">
  
  
  
  
  
  
    <p class="title logo__title">Machine Learning for Natural Hazards</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="README.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Background and Refresher</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_intro.html">Introduction to Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_linalg.html">Linear Algebra refresher</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_gradopt.html">Gradient-based optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_linreg.html">Linear and Logistic Regression</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Neural Networks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="05_nn.html">Basics of Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_nn.html">More on Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_bestpractice.html">Best practices in the training of Machine Learning models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Convolutional Neural Networks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="08_gradopt1.html">More on gradient-based optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_cnn.html">Convolutional Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_cnnarch.html">CNNs Popular Architectures</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/ashokdahal/GeoAI4Haz" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/ashokdahal/GeoAI4Haz/issues/new?title=Issue%20on%20page%20%2F19_implicit.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/19_implicit.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Implicit neural networks</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fixed-point-iterations">Fixed point iterations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further reading</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="implicit-neural-networks">
<h1>Implicit neural networks<a class="headerlink" href="#implicit-neural-networks" title="Link to this heading">#</a></h1>
<p>Neural networks consists of a sequence of consecutive operations that are typically defined explicitly. An explicit operation is one that computes the output directly
from a sequence of explicit operations applied to the input. A simple example is a feed-forward MLP, where the transition from one layer to the next is done by the following sequence
of operations</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
z_i &amp; = W_iz_{i-1} + b_i \\
a_i &amp; = \sigma_i(z_i)
\end{aligned}
\end{split}\]</div>
<p>Additionally one could add operations like batch normalization and max pooling, all of which are given explicitly. Alternatively, two variables can be related
via an implicit equation. A simple example of an explicit function versus an implicit function is <span class="math notranslate nohighlight">\(y = x^2\)</span> versus <span class="math notranslate nohighlight">\(x^2 + y^2 = 1\)</span>. From the second example it
becomes clear why implicit functions are sometimes favorable, since the implicit function <span class="math notranslate nohighlight">\(x^2 + y^2 = 1\)</span> has an explicit counterpart with two equations, namely
<span class="math notranslate nohighlight">\(y = \sqrt{1 - x^2}\)</span> and <span class="math notranslate nohighlight">\(y = -\sqrt{1 - x^2}\)</span>. In a more abstract fashion, we can write an explicit equation as</p>
<div class="math notranslate nohighlight">
\[
  y = f(x),
\]</div>
<p>and an implicit equation as</p>
<div class="math notranslate nohighlight">
\[
  f(x, y) = 0.
\]</div>
<p>Neural networks can be defined implicitly as well through the concept of implicit layers and were introduced under the name <em>Deep Equilibrium Models (DEQ)</em>. This concept
is a bit abstract but the nice thing about this paradigm is that the memory requirements for deep networks are constant. To understand this concept we need to cover
two fundamental concepts:</p>
<ul class="simple">
<li><p>Implicit functions and the implicit function theorem. Taking derivatives of explicit functions is easy, since we have an explicit relation of the output with respect
to the input, and we can compute <span class="math notranslate nohighlight">\(\frac{dy}{dx}\)</span> in a straightforward manner. However, if <span class="math notranslate nohighlight">\(y\)</span> is only given through <span class="math notranslate nohighlight">\(f(x, y) = 0\)</span> then computing the derivative is
less straightforward.</p></li>
<li><p>Fixed point iterations. Fixed point iterations are iterations of the form <span class="math notranslate nohighlight">\(x_{k+1} = \mathcal{F}(x_k)\)</span> and we call a vector <span class="math notranslate nohighlight">\(x_{\star}\)</span> a <em>fixed point of <span class="math notranslate nohighlight">\(\mathcal{F}\)</span></em>
if <span class="math notranslate nohighlight">\(x_{\star} = \mathcal{F}(x_{\star})\)</span>. DEQs are based on the idea that the layers of a neural network will eventually reach a fixed point.</p></li>
</ul>
<section id="fixed-point-iterations">
<h2>Fixed point iterations<a class="headerlink" href="#fixed-point-iterations" title="Link to this heading">#</a></h2>
<p>Consider the following fixed point iteration</p>
<div class="math notranslate nohighlight">
\[
  z_{k+1} = \tanh(Wz_k + b + x).
\]</div>
<p>This is essentially repeated application of one layer of a neural network with weight matrix <span class="math notranslate nohighlight">\(W\)</span>, bias <span class="math notranslate nohighlight">\(b\)</span>, some input <span class="math notranslate nohighlight">\(x\)</span> and activation function <span class="math notranslate nohighlight">\(\tanh\)</span>. Assuming for now that
a fixed point actually exists we iterate until convergence, i.e. <span class="math notranslate nohighlight">\(z_{\star} = \mathcal{F}(z_{\star})\)</span> up to some tolerance. Alternatively, we can write the above equation as</p>
<div class="math notranslate nohighlight">
\[
  z - \tanh(Wz + b + x) = 0,
\]</div>
<p>where the function is now implicitly defined. Defining</p>
<div class="math notranslate nohighlight">
\[
  g(x, z) := z - \tanh(Wz + b + x),
\]</div>
<p>the goal is now to solve the root finding problem</p>
<div class="math notranslate nohighlight">
\[
  g(x, z_{\star}(x)) = 0,
\]</div>
<p>where <span class="math notranslate nohighlight">\(z_{\star}(x)\)</span> denotes the solution depending on <span class="math notranslate nohighlight">\(x\)</span>. Let the solution to this problem be given by <span class="math notranslate nohighlight">\(z_{\star}(x)\)</span> and assume we want
to compute <span class="math notranslate nohighlight">\(\frac{dz_{\star}(x)}{dx}\)</span> (note that we could choose to differentiate through any parameter, for example the weight matrix,
but this is just for illustrative purposes). Since we only have access to <span class="math notranslate nohighlight">\(z_{\star}\)</span> through the equation <span class="math notranslate nohighlight">\(g(x, z_{\star}(x)) = 0\)</span> need to
differentiate through this equation to obtain <span class="math notranslate nohighlight">\(\frac{dz_{\star}(x)}{dx}\)</span>. This yields:</p>
<div class="math notranslate nohighlight">
\[
  \frac{\partial}{\partial x}g(x, z_{\star}(x)) = \frac{\partial g(x, z_{\star})}{\partial x} +  \frac{\partial g(x, z_{\star})}{\partial z_{\star}}\frac{\partial z_{\star}(x)}{\partial x} = 0
\]</div>
<p>This equation allows us to solve for <span class="math notranslate nohighlight">\(\frac{dz_{\star}(x)}{dx}\)</span> as follows</p>
<div class="math notranslate nohighlight">
\[
  \frac{\partial z_{\star}(x)}{\partial x} = -\left(\frac{\partial g(x, z_{\star})}{\partial z_{\star}}\right)^{-1}\frac{\partial g(x, z_{\star})}{\partial x}
\]</div>
<p>The main question here is whether existence is guaranteed. The <em>implicit function theorem</em> states that if a fixed point exists and the function
<span class="math notranslate nohighlight">\(g\)</span> is differentiable with non-singular Jacobian around <span class="math notranslate nohighlight">\(z_{\star}\)</span> there exists a unique function <span class="math notranslate nohighlight">\(z_{\star}(x)\)</span>. The key point here is that
one can differentiate through <span class="math notranslate nohighlight">\(z_{\star}\)</span> without needing to differentiate through the solver used to obtain the fixed point. This saves a huge amount of
memory that would otherwise be needed in order to perform backpropagation.</p>
<p>This observation has led to the development of the <em>Deep Equilibrium Network</em>. This network has the following structure:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
z_1 &amp; = 0 \\
z_i &amp; = \sigma_i(Wz_i + Ux + b_i), \quad i=1,\ldots, k \\
h(x) &amp; = W_kz_k + b_k
\end{aligned}
\end{split}\]</div>
<p>As we can see, DEQs apply a fixed point iteration to a single layer of a neural network. The question is whether this fixed point iteration
actually converges: It could also blow-up or oscillate. It turns out that in general the fixed point iteration converges. As you can probably guess
at this point, the fixed point iteration is solved using implicit differentiation, thereby bypassing the need to store any information necessary for
the backward pass. This way one can build an extremely deep network. If we now want to update the weights of the neural network we need to evaluate the
partial derivative with respect to <span class="math notranslate nohighlight">\(W\)</span>. Given that <span class="math notranslate nohighlight">\(z_{\star}\)</span> is a fixed point we have</p>
<div class="math notranslate nohighlight">
\[
  z_{\star} = f(x, z_{\star}) \: \Leftrightarrow \: \frac{\partial z_{\star}}{\partial W} = \frac{\partial f(x, z_{\star})}{\partial W}
\]</div>
<p>Computing <span class="math notranslate nohighlight">\(\frac{\partial f(x, z_{\star})}{\partial W}\)</span> via implicit differentiation and rearranging terms gives</p>
<div class="math notranslate nohighlight">
\[
  \frac{\partial z_{\star}}{\partial W} =  \left(I - \frac{\partial f(x, z_{\star})}{\partial z_{\star}}\right)^{-1}\frac{\partial f(x, z_{\star})}{\partial W}
\]</div>
<p>Backpropagation actually implements the transpose of this expression, i.e.:</p>
<div class="math notranslate nohighlight">
\[
  \left(\frac{\partial z_{\star}}{\partial W}\right)^Ty =  \left(\frac{\partial f(x, z_{\star})}{\partial W}\right)^T\left(I - \frac{\partial f(x, z_{\star})}{\partial z_{\star}}\right)^{-T}y,
\]</div>
<p>where <span class="math notranslate nohighlight">\(y\)</span> is some vector we apply the gradient to. Evaluating the gradient is now a two-step process:</p>
<ul class="simple">
<li><p>Evaluate <span class="math notranslate nohighlight">\(\left(I - \frac{\partial f(x, z_{\star})}{\partial z_{\star}}\right)^{-1}y\)</span>. Since this matrix tends to be large we do not evaluate
the inverse directly, but rather solve the linear system
$<span class="math notranslate nohighlight">\(
y = \left(I - \frac{\partial f(x, z_{\star})}{\partial z_{\star}}\right)g \quad \Leftrightarrow \quad g = \frac{\partial f(x, z_{\star})}{\partial z_{\star}}g + y.
\)</span>$</p></li>
<li><p>Compute
$<span class="math notranslate nohighlight">\(
\left(\frac{\partial z_{\star}}{\partial W}\right)^Ty = \left(\frac{\partial f(x, z_{\star})}{\partial W}\right)^Tg
\)</span>$</p></li>
</ul>
<p>So far we have considered a rather simple model for the DEQ. We have assumed a constant weight <span class="math notranslate nohighlight">\(W\)</span> accross the layers and have assumed a simple
feed-forward model. It turns out that a feed-forward neural network with constant weights accross the layers is actually equivalent to a neural network
with a layer-dependent matrix, which is summarized in the following theorem by <a class="reference external" href="https://arxiv.org/pdf/1909.01377.pdf">Bai et al., 2019</a>:</p>
<p>Consider a traditional <span class="math notranslate nohighlight">\(L\)</span>-layer MLP</p>
<div class="math notranslate nohighlight">
\[
  z_{i+1} = \sigma_{i}(W_iz_i + b_i), \quad i=0,\ldots,L-1, \quad z_0 = x.
\]</div>
<p>This network is equivalent to the following weight-tied network of equivalent depth:</p>
<div class="math notranslate nohighlight">
\[
  \tilde{z}_{i+1} = \tilde{\sigma}(W_zz_i + \tilde{b} + Ux), \quad i=0, \ldots, L-1, \quad \tilde{z}_{0} = (0, \ldots, 0)^T
\]</div>
<p>We prove the theorem for the case <span class="math notranslate nohighlight">\(L = 4\)</span>, but it extends to general <span class="math notranslate nohighlight">\(L\)</span>. Define the matrices</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  W_z = \begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 \\ W_1 &amp; 0  &amp; 0 &amp; 0 \\ 0 &amp; W_2 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; W_{3} &amp; 0 \end{bmatrix}, \:
U = \begin{bmatrix} W_0 \\ 0 \\ 0 \\ 0 \end{bmatrix}, \: \tilde{b} = \begin{bmatrix} b_0 \\ b_1 \\ b_2 \\ b_3 \end{bmatrix}, \: \tilde{\sigma} = \begin{bmatrix} \sigma_0 \\ \sigma_1 \\ \sigma_2 \\ \sigma_3 \end{bmatrix}.
\end{split}\]</div>
<p>Then after one iteration we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \tilde{z}_1 = \tilde{\sigma}(W_z\tilde{z}_0 + Ux + \tilde{b}) = \begin{bmatrix} \sigma_0(W_0x + b_0) \\ \sigma_1(b_1) \\ \sigma_2(b_2) \\ \sigma_3(b_3) \end{bmatrix} 
= \begin{bmatrix} z_0 \\ \sigma_1(b_1) \\ \sigma_2(b_2) \\ \sigma_3(b_3) \end{bmatrix}.
\end{split}\]</div>
<p>For the second iteration we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  W_z\tilde{z_1} = \begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 \\ W_1 &amp; 0  &amp; 0 &amp; 0 \\ 0 &amp; W_2 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; W_{3} &amp; 0 \end{bmatrix}\begin{bmatrix} z_0 \\ \sigma_1(b_1) \\ \sigma_2(b_2) \\ \sigma_3(b_3) \end{bmatrix} = \begin{bmatrix} 0 \\ W_1z_0 \\ W_2\sigma_2(b_2) \\ W_3\sigma_2(b_2) \end{bmatrix}
\end{split}\]</div>
<p>and hence</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \tilde{z}_{2} = \tilde{\sigma}(W_zz_1 + \tilde{b} + Ux) = \begin{bmatrix} \sigma_0(W_0x + b_0) \\ \sigma_1(W_1z_0 + b_1) \\ \sigma_2(W_2\sigma_1(b_1) + b_2) \\ \sigma_3(W_3\sigma_2(b_2) + b_3) \end{bmatrix} 
= \begin{bmatrix} z_0 \\ z_1 \\ \sigma_2(W_2\sigma_1(b_1) + b_2) \\ \sigma_3(W_3\sigma_2(b_2) + b_3) \end{bmatrix}. 
\end{split}\]</div>
<p>Similarly, for the next layer we obtain</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 \\ W_1 &amp; 0  &amp; 0 &amp; 0 \\ 0 &amp; W_2 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; W_{3} &amp; 0 \end{bmatrix}\begin{bmatrix} z_0 \\ z_1 \\ \sigma_2(W_2\sigma_1(b_1) + b_2) \\ \sigma_3(W_3\sigma_2(b_2) + b_3) \end{bmatrix} = \begin{bmatrix} 0 \\ W_1z_0 \\ W_2z_1 \\ W_3\sigma_2(W_2\sigma_1(b_1) + b_2) \end{bmatrix}
\end{split}\]</div>
<p>which leads to</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \tilde{z}_3 = \begin{bmatrix} \sigma_0(W_0x + b_0) \\ \sigma_1(W_1z_0 + b_1) \\ \sigma_2(W_2z_1 + b_2) \\ \sigma_3(W_3\sigma_2(W_2\sigma_1(b_1) + b_2) + b_3) \end{bmatrix} = \begin{bmatrix} z_1 \\ z_2 \\ z_3 \\ \sigma_3(W_3\sigma_2(W_2\sigma_1(b_1) + b_2) + b_3) \end{bmatrix}
\end{split}\]</div>
<p>Then, finally,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 \\ W_1 &amp; 0  &amp; 0 &amp; 0 \\ 0 &amp; W_2 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; W_{3} &amp; 0 \end{bmatrix}\begin{bmatrix} z_1 \\ z_2 \\ z_3 \\ \sigma_3(W_3\sigma_2(W_2\sigma_1(b_1) + b_2) + b_3) \end{bmatrix} = \begin{bmatrix} 0 \\ W_1z_1 \\ W_2z_2 \\ W_3z_3 \end{bmatrix}
\end{split}\]</div>
<p>and hence</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \tilde{z}_4 = \begin{bmatrix} \sigma_0(W_0x + b_0) \\ \sigma_1(W_1z_0 + b_1) \\ \sigma_2(W_2z_1 + b_2) \\ \sigma_3(W_3z_2 + b_3) \end{bmatrix} = \begin{bmatrix} z_1 \\ z_2 \\ z_3 \\ z_4 \end{bmatrix}.
\end{split}\]</div>
<p>Moreover, note that we have only used a single layer DEQ as opposed to the multi-layer architecture that is typical for powerful neural networks. However,
any deep neural network can be represented as a deep neural network. The argument is as follows. Assume that construct a two-layer network <span class="math notranslate nohighlight">\(g_2(g_1(x))\)</span>. This can
be posed a single layer DEQ using the following relation:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  f(z, x) = f\left( \begin{bmatrix} z_1 \\ z_2 \end{bmatrix}, x\right) = \begin{bmatrix} g(x) \\ g(z_1) \end{bmatrix}
\end{split}\]</div>
<p>That is, the complexity of the extra layer can simply be added by concatenating the two layers to make a single layer neural network. The same argument
holds for stacking DEQs: a single DEQ can model any number of stacked DEQs.</p>
<p>Finally, we can increase the complexity of the DEQ by substituting the simple feed-forward neural network with any sequence of operations, including
convolutions, normalizations, grouping and skip connections.</p>
</section>
<section id="further-reading">
<h2>Further reading<a class="headerlink" href="#further-reading" title="Link to this heading">#</a></h2>
<p>These notes are essentially a summary of the following tutorial, specifically chapters 1 and 4:</p>
<ul class="simple">
<li><p><a class="reference external" href="http://implicit-layers-tutorial.org/">Implicit layer tutorial</a></p></li>
</ul>
<p>Below is the paper introducing Deep Equilibrium Models (DEQ):</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/pdf/1909.01377.pdf">Bai et al., 2019</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fixed-point-iterations">Fixed point iterations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further reading</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Ashok Dahal, Modified from the Content of Matteo Ravasi, KAUST
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>