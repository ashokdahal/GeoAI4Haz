
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Deep learning for Inverse Problems &#8212; Machine Learning for Natural Hazards</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '17_deepinv';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="README.html">
  
  
  
  
  
  
    <p class="title logo__title">Machine Learning for Natural Hazards</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="README.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Background and Refresher</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_intro.html">Introduction to Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_linalg.html">Linear Algebra refresher</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_gradopt.html">Gradient-based optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_linreg.html">Linear and Logistic Regression</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Neural Networks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="05_nn.html">Basics of Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_nn.html">More on Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_bestpractice.html">Best practices in the training of Machine Learning models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Convolutional Neural Networks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="08_gradopt1.html">More on gradient-based optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_cnn.html">Convolutional Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_cnnarch.html">CNNs Popular Architectures</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/ashokdahal/GeoAI4Haz" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/ashokdahal/GeoAI4Haz/issues/new?title=Issue%20on%20page%20%2F17_deepinv.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/17_deepinv.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Deep learning for Inverse Problems</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-driven-or-learned-regularization-of-inverse-problems">Data-driven or learned regularization of inverse problems</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learned-solvers">Learned solvers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#variants-of-learned-solvers">Variants of learned solvers</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="deep-learning-for-inverse-problems">
<h1>Deep learning for Inverse Problems<a class="headerlink" href="#deep-learning-for-inverse-problems" title="Link to this heading">#</a></h1>
<p>The field of inverse problem has experienced a renaissance in the last decade thanks to the recent advances in Deep Learning.
Whilst solid theories exist for the solution of linear (or nonlinear) inverse problems, in practice one is always faced with
problems that are ill-posed by nature, i.e. many solutions exist that can match data equally well. This is where for
long time the inverse problem community has spent time and resources to identify mitigating strategies to reduce the so-called
nullspace of an inverse problem by means of prior information. Similarly, for long time the optimization community has developed
iterative solvers that can provide solutions to convex or non-convex functionals by requiring only access to function and gradient
evaluations of the functional of interest. In this lecture we will discuss where and how Deep Learning may be of great help in the
solution of inverse problems.</p>
<section id="data-driven-or-learned-regularization-of-inverse-problems">
<h2>Data-driven or learned regularization of inverse problems<a class="headerlink" href="#data-driven-or-learned-regularization-of-inverse-problems" title="Link to this heading">#</a></h2>
<p>To begin, let’s consider the solution of an inverse problem of the form:</p>
<p><span class="math notranslate nohighlight">\(\mathbf{d}^{obs}=g(\mathbf{m})\)</span></p>
<p>or</p>
<p><span class="math notranslate nohighlight">\(\mathbf{d}^{obs} = \mathbf{Gm}\)</span></p>
<p>where <span class="math notranslate nohighlight">\(g\)</span> or <span class="math notranslate nohighlight">\(\mathbf{G}\)</span> is the known modelling operator, <span class="math notranslate nohighlight">\(\mathbf{m}\)</span> are the unknown model parameters,
and <span class="math notranslate nohighlight">\(\mathbf{d}^{obs}\)</span> are the observed data. As previously mentioned, in many (geo)scientific applications
the operator may be ill-posed and prior knowledge is required to obtain a plausible solution (not just one
of the many that matches the data). In classical inverse problem theory this can be achieved as follows:</p>
<ul class="simple">
<li><p>Regularization: <span class="math notranslate nohighlight">\(J = ||\mathbf{d}^{obs}-g(\mathbf{m})||_p^p + \lambda ||r(\mathbf{m})||_p^p\)</span>
where <span class="math notranslate nohighlight">\(r\)</span> is a function that tries to penalize some features of the model that we are not interested in. Classical
choices of <span class="math notranslate nohighlight">\(r\)</span> are linear operators such as the identity matrix (this type of regularization is called Tikhonov regularization
and favours solution with small L2 norm - <span class="math notranslate nohighlight">\(p=2\)</span>) or the second derivative of laplacian operator (this type of regularization
favour smooth solutions). Alternatively, one could choose a linear or nonlinear projection that transforms the model into a
domain where the solution is sparse; by choosing <span class="math notranslate nohighlight">\(p=1\)</span>, one can estimate the sparsest model that at the same time matches the data.</p></li>
<li><p>Preconditioning: <span class="math notranslate nohighlight">\(J = ||\mathbf{d}^{obs}-g(p(\mathbf{z}))||_p^p + \lambda ||\mathbf{z}||_p^p\)</span>
where by performing a change of variable (<span class="math notranslate nohighlight">\(\mathbf{m}=p(\mathbf{z})\)</span>) the inverse problem is now solved in a transformed domain, and
<span class="math notranslate nohighlight">\(p\)</span> is a function that filters the solution <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> in such a way that favourable features of the model are enhanced. As an example,
a smoothing operator can be used to produce smooth solution (note how this differs from the previous approach where smooth solutions
could be constructed by penalizing roughness in the solution by means of second derivatives).</p></li>
</ul>
<p>A common feature of these two families of approaches is that we as user are requested to select the regularizer or preconditioner for
the problem at hand. This could be a difficult task and usually requires a lot of trial-and-error before a good choice is made for a
specific problem. Alternatively, one could define a projection that reduces the dimensionality of the space in which we wish to find the solution (i.e., <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^{N_x}, 
\mathbf{z} \in \mathbb{R}^{N_z}\)</span> with <span class="math notranslate nohighlight">\(N_z &lt;&lt; N_x\)</span>). This approach reminds us of the dimensionality reduction techniques discusses in this <a class="reference internal" href="13_dimred.html"><span class="std std-doc">lecture</span></a>
and the choice of the method used to identify a representative latent space can be arbitrary (i.e., a simple linear transformation like PCA or a complex nonlinear
transformation like that induced by an Autoencoder or a GAN). A clear advantage of such an approach is that the user is not required to define
a transform upfront. Provided availability of training dataset in the form of a representative set of solutions <span class="math notranslate nohighlight">\(M = (\mathbf{m}^{&lt;1&gt;}, 
\mathbf{m}^{&lt;2&gt;}, ..., \mathbf{m}^{&lt;N&gt;})\)</span>, the best data-driven transformation can be identified that suits the problem at hand.</p>
<p>Before we get more into the details of such an approach, it is important to make a few remarks. This approach lies in between
classical approaches in inverse problem theory and supervised learning approaches in that:</p>
<ul class="simple">
<li><p>classical inverse problems: only the modelling operator <span class="math notranslate nohighlight">\(g/\mathbf{G}\)</span> and <em>one instance</em> of data <span class="math notranslate nohighlight">\(\mathbf{d}^{obs}\)</span> are available. Prior information comes from our
knowledge of the expected solution (or its probability distribution), but no set of solutions are available when solving the problem;</p></li>
<li><p>supervised learning: pairs of models and associated observations <span class="math notranslate nohighlight">\((\mathbf{m}^{&lt;i&gt;}, \mathbf{d}^{obs,&lt;i&gt;})\)</span> are available upfront
(or a set of models <span class="math notranslate nohighlight">\((\mathbf{m}^{&lt;i&gt;}\)</span> from which the associated observations can be synthetically created via the modelling operator). A data-driven model (e.g., a NN)
is then trained to find the mapping between data and models. Note that the modelling operator is not actively used in the training process;</p></li>
<li><p>learned regularization: a set of models <span class="math notranslate nohighlight">\((\mathbf{m}^{&lt;i&gt;}\)</span> is available upfront, which are used to find a latent representation. The inverse problem is subsequently solved for <em>one instance</em> of data <span class="math notranslate nohighlight">\(\mathbf{d}^{obs}\)</span>
using the learned regularizer (or preconditioner) and the physical modelling operator.</p></li>
</ul>
<p>The key idea of solving inverse problems with learned regularizers is therefore to split the problem into two subsequent tasks, where the first is concerned with the prior
and the latter with the modelling operator (this is different from the supervised learning approach where the two are learned together):</p>
<ul>
<li><p>Learning process: a nonlinear model is trained to identify a representative latent space for the set of available solutions. Such model can be an AE (or VAE) network:</p>
<div class="math notranslate nohighlight">
\[
    \underset{\mathbf{e}_\theta, \mathbf{d}_\phi} {\mathrm{argmin}} \; \frac{1}{N_s}\sum_i \mathscr{L}(\mathbf{m}^{(i)}, d_\phi(e_\theta(\mathbf{m}^{(i)}))) 
    \]</div>
<p>or a GAN network</p>
<div class="math notranslate nohighlight">
\[
    arg \; \underset{g_\theta} {\mathrm{min}} \; \underset{d_\phi} {\mathrm{max}} \; \frac{1}{N_s}\sum_i \mathscr{L}_{adv}(\mathbf{m}^{(i)})
    \]</div>
</li>
<li><p>Inversion: Once the training process is finalized, the decoder (or generator) is used as a nonlinear preconditioner to the solution of the inverse problem as follows:</p>
<div class="math notranslate nohighlight">
\[
    AE: \mathbf{m} = d_\phi(\mathbf{z}) = p(\mathbf{z}) \quad GAN: \mathbf{m} = g_\theta(\mathbf{z}) = p(\mathbf{z}) 
    \]</div>
<p>such that the inverse problem becomes:</p>
<div class="math notranslate nohighlight">
\[
    J = ||\mathbf{d}^{obs}-g(p(\mathbf{z}))||_p^p + \lambda ||\mathbf{z}||_p^p
    \]</div>
<p>This problem can be now solved using a nonlinear solver of choice, where the gradient can be easily computed using the same set of tools that we employed in the
training process of neural networks, namely backpropagation:</p>
<div class="math notranslate nohighlight">
\[
    \frac{\partial J}{\partial \mathbf{z}} = \frac{\partial J}{\partial g} \frac{\partial g}{\partial p} \frac{\partial p}{\partial \mathbf{z}}
    \]</div>
<p>where <span class="math notranslate nohighlight">\(\partial J / \partial g\)</span> is the derivative of the loss function over the predicted data, <span class="math notranslate nohighlight">\(\partial g / \partial p\)</span> is the derivative of the physical modelling
operator, and <span class="math notranslate nohighlight">\(\partial p / \partial \mathbf{z}\)</span> is the derivative of the decoder of the pretrained AE (or that of the generator of the pretrained GAN) over the input.</p>
</li>
</ul>
<p>Finally, it is worth noting that when an autoencoder is used to find a representative latent space, alternatively a regularized problem of this form can be solved:</p>
<div class="math notranslate nohighlight">
\[
J = ||\mathbf{d}^{obs}-g(\mathbf{m})||_p^p + \lambda ||\mathbf{m} - d_\phi(e_\theta(\mathbf{m})) ||_p^p
\]</div>
<p>where the regularization terms ensures that the autoencoder can recreate the estimated model. This ensures that the solution lies in the manifold of the set of plausible solutions used
to train the AE network.</p>
</section>
<section id="learned-solvers">
<h2>Learned solvers<a class="headerlink" href="#learned-solvers" title="Link to this heading">#</a></h2>
<p>In the previous section we have discussed the solution of linear (or nonlinear) inverse problems from a high-level perspective. In fact, we purposely decided to avoid any discussion
regarding the numerical aspects of solving any of the cost functions <span class="math notranslate nohighlight">\(J\)</span>. In practice, real-life problems may target model spaces that contain millions (or even billions) of variables and the
same usually applies for the observation vector. Under these conditions, iterative solvers similar to those presented <a class="reference internal" href="03_gradopt.html"><span class="std std-doc">here</span></a> and <a class="reference internal" href="08_gradopt1.html"><span class="std std-doc">here</span></a> are
therefore the only viable option.</p>
<p>An iterative solver can be loosely expressed as a nonlinear function <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> of this form:</p>
<div class="math notranslate nohighlight">
\[
\hat{\mathbf{m}} = \mathcal{F}(\mathbf{d}^{obs}, \mathbf{m}_0, g/\mathbf{G})
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{m}_0\)</span> is an initial guess. The vanilla gradient descent algorithm can be more explicitly described by the following update rule:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{m}_{i+1} = \mathbf{m}_i - \alpha \frac{\partial J}{\partial \mathbf{m}} | _ {\mathbf{m}=\mathbf{m}_i} (\mathbf{d}^{obs}, \mathbf{m}, g/\mathbf{G})
\]</div>
<p>which we can <em>unroll</em> for a number of iterations and write as:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{m}_{2} = \mathbf{m}_0 - \alpha \frac{\partial J}{\partial \mathbf{m}} | _ {\mathbf{m}=\mathbf{m}_0} - \alpha \frac{\partial J}{\partial \mathbf{m}} | _ {\mathbf{m}=\mathbf{m}_1}
\]</div>
<p>This expression clearly shows that the solution of an iterative solver at a given iteration is a simple weighted summation of the intermediate gradients that are subtracted from
the initial guess <span class="math notranslate nohighlight">\(\mathbf{m}_0 \)</span>. Similarly, more advanced solvers like the linear or nonlinear conjugate gradient algorithm take into account the past gradients at each iteration,
however they still apply simple linear scalings to the gradients to produce the final solution.</p>
<p>The mathematical community has recently started to investigate a new family of iterative solvers, called learned solvers. The key idea lies in the fact that a linear combination of gradients may not be the
best choice (both in terms of convergence speed and ultimate quality of the solution). An alternative update rule of this form</p>
<div class="math notranslate nohighlight">
\[
\mathbf{m}_{i+1} = \mathbf{m}_i - f_\theta \left( \frac{\partial J}{\partial \mathbf{m}} | _ {\mathbf{m}=\mathbf{m}_i}\right)
\]</div>
<p>may represent a better choice. However, a question may arise at this point: how do we choose the nonlinear project <span class="math notranslate nohighlight">\(f_\theta\)</span> that we are going to apply to the gradients at each step?
Learned iterative solvers, as the name implies, learn this mapping. More specifically, assuming availability of pairs of models and associated observations <span class="math notranslate nohighlight">\((\mathbf{m}^{&lt;i&gt;}, \mathbf{d}^{obs,&lt;i&gt;})\)</span>,
a supervised learning process is setup such that an iterative solver with <span class="math notranslate nohighlight">\(N_it\)</span> iterations is tasked to learn the mapping from data to models. Let’s take a look at the schematic below
to better understand how this works:</p>
<p><img alt="LEARNEDSOLV" src="_images/learnedsolv.png" /></p>
<p>A learned iterative solver can be seen as an unrolled neural network where each element takes as input the current model estimate and its gradient and produces an updated version
of the model. To keep the model capacity low, each unit shares weights like in classical RNN and each update can be compactly written as:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{m}_i = f_\theta(\mathbf{x}_i), \qquad \mathbf{x}_i = \mathbf{m}_{i-1} \oplus \frac{\partial J}{\partial \mathbf{m}}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\oplus\)</span> indicates concatenation over the channel axis (assuming that model and gradient are N-dimensional tensors). Depending on the problem and type of data
<span class="math notranslate nohighlight">\(f_\theta\)</span> can be chosen to be any network architecture, from a simple FF block, to a stack of FF blocks, or even a convolutional neural network.
Moreover, given that we have access to the solution, the loss function is set up as follows:</p>
<div class="math notranslate nohighlight">
\[
\underset{f_\theta} {\mathrm{arg min}} \; \frac{1}{N_s}\sum_{i=1}^{N_s} \sum_{j=1}^{N_{it}} w_j \mathscr{L}(\mathbf{m}_j^{(i)}, \mathbf{m})
\]</div>
<p>where each estimate is compared to the true model. Since early iterations may be worse, an exponentially increasing weight may be used to downweight their contributions over the estimates
as later iterations of the unrolled solver. Finally, once the learning process is finalized, inference can be simply performed by evaluation a single forward pass of the network for <em>one instance</em> of data <span class="math notranslate nohighlight">\(\mathbf{d}^{obs}\)</span> and
a chosen initial guess.</p>
<p>To conclude, it is important to answer the following question: why learned solvers are better than pure vanilla supervised learning?</p>
<p>The key difference between these two approaches lies in how they decide to use the knowledge of the modelling operator <span class="math notranslate nohighlight">\(g/\mathbf{G}\)</span>. Whilst traditional supervised
learning approaches may use the modelling operator in the process of generating training data whilst ignoring it during training, learned iterative solvers integrate the
modelling operator in the learning process. Two benefits may arise from this choice: generalization of the trained network over unseen modelling operator and
increased robustness to noise in the data.</p>
<section id="variants-of-learned-solvers">
<h3>Variants of learned solvers<a class="headerlink" href="#variants-of-learned-solvers" title="Link to this heading">#</a></h3>
<p>The structure of the learned solver discussed above closely resembles the method proposed by <a class="reference external" href="https://iopscience.iop.org/article/10.1088/1361-6420/aa9581/meta">Adler and Öktem</a> in 2017.
A number of variants have been suggested in the literature in the following years:</p>
<p><strong>Learned solver with memory</strong></p>
<p>Adler and Öktem further propose to include a memory variable <span class="math notranslate nohighlight">\(\mathbf{s}\)</span>. This takes inspiration from conventional solvers that use past gradients (or memory) to obtain more informed update directions.</p>
<p><img alt="LEARNEDSOLV1" src="_images/learnedsolv1.png" /></p>
<p>The model update can be therefore written as follows:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{y}_i = f_\theta(\mathbf{x}_i), \qquad \mathbf{x}_i = \mathbf{m}_{i-1} \oplus \frac{\partial J}{\partial \mathbf{m}} \oplus \mathbf{s}_{i-1} 
\qquad \mathbf{y}_i = \mathbf{m}_i \oplus \mathbf{s}_i
\]</div>
<p><strong>Recurrent Inference Machines (RIMs)</strong></p>
<p>RIMs closely resemble the second learned solver of Adler and Öktem. They however differ in the design on the network block and the fact that similarly to RNNs two set of parameters
are used instead of one, <span class="math notranslate nohighlight">\(f_\theta\)</span> and <span class="math notranslate nohighlight">\(f'_\phi\)</span>.</p>
<p>The model update can be therefore written as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{s}_i &amp;= f'_\phi (\mathbf{z}_i) , \qquad \mathbf{z}_i = \boldsymbol \eta_{i-1} \oplus \frac{\partial J}{\partial \mathbf{m}} \oplus \mathbf{s}_{i-1} \\
\boldsymbol \eta_i &amp;= \boldsymbol \eta_{i-1} + f_\theta(\mathbf{x}_i), \qquad \mathbf{x}_i = \boldsymbol \eta_{i-1} \oplus \frac{\partial J}{\partial \mathbf{m}} \oplus \mathbf{s}_i
\end{aligned}
\end{split}\]</div>
<p>where a new variable <span class="math notranslate nohighlight">\(\boldsymbol \eta\)</span> has been introduced. This is the unscaled output and is connected to the model via a nonlinear activation function <span class="math notranslate nohighlight">\(\sigma\)</span> that is in change of
defining a range of allowed values: <span class="math notranslate nohighlight">\(\mathbf{z} = \sigma ( \boldsymbol \eta)\)</span>.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-driven-or-learned-regularization-of-inverse-problems">Data-driven or learned regularization of inverse problems</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learned-solvers">Learned solvers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#variants-of-learned-solvers">Variants of learned solvers</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Ashok Dahal, Modified from the Content of Matteo Ravasi, KAUST
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>